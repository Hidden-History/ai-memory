# ðŸ”­ Langfuse LLM Observability

Langfuse integration for the AI Memory Module. Traces every memory pipeline operation â€” from hook capture through LLM classification â€” so you can see exactly what your memory system is doing, how long each step takes, and where issues occur.

Uses [Langfuse v3](https://langfuse.com/) self-hosted with the Python SDK for trace ingestion.

---

## Overview

When Langfuse observability is enabled, the AI Memory Module traces every memory operation across a 9-step pipeline. This gives you visibility into:

- **Hook execution** â€” which hooks fired, what they captured, how long they took
- **Pipeline steps** â€” capture, logging, detection, scanning, chunking, embedding, storage, enqueuing, and classification
- **Session grouping** â€” all traces from a single Claude Code session are linked together
- **Classification outcomes** â€” which LLM provider classified each memory, with confidence scores
- **Buffer health** â€” trace buffer depth, flush latency, and error rates

All trace data is stored in your self-hosted Langfuse instance â€” nothing leaves your infrastructure.

---

## Prerequisites

- **AI Memory Module** installed and running (see [INSTALL.md](../INSTALL.md))
- **Docker Compose** with the Langfuse overlay file (`docker-compose.langfuse.yml`)
- **32 GiB RAM** recommended (Langfuse adds 6 services to the existing 8)
- **8 CPU cores** recommended for the combined 14-service stack

---

## Setup Guide

### Automated Setup (Recommended)

The `langfuse_setup.sh` script handles all setup steps:

```bash
cd /path/to/ai-memory
./scripts/langfuse_setup.sh
```

This runs three phases:

1. **Generate Secrets** (`--generate-secrets`): Creates cryptographic secrets, Langfuse project initialization variables, and MinIO S3 bucket configuration. All values are written to `docker/.env`.

2. **Start Services** (`--start`): Starts the 7 Langfuse Docker services (Web UI, Worker, PostgreSQL, ClickHouse, Redis, MinIO, Trace Flush Worker) using the `langfuse` profile.

3. **Health Check & Model Registration** (`--health-check`): Waits for all services to be healthy, registers custom LLM models (Ollama, OpenRouter), and prints a connection summary.

You can run individual phases:

```bash
./scripts/langfuse_setup.sh --generate-secrets   # Only generate secrets
./scripts/langfuse_setup.sh --start              # Only start services
./scripts/langfuse_setup.sh --health-check       # Only health check + model registration
```

### Manual Setup

If you prefer to configure manually:

#### 1. Generate Secrets

Add these to `docker/.env`:

```bash
# Langfuse Core
LANGFUSE_DB_PASSWORD=<random 32-char hex>
LANGFUSE_NEXTAUTH_SECRET=<random 32-char hex>
LANGFUSE_SALT=<random 32-char hex>
LANGFUSE_ENCRYPTION_KEY=<random 64-char hex>

# ClickHouse
LANGFUSE_CLICKHOUSE_PASSWORD=<random 32-char hex>

# MinIO S3
LANGFUSE_S3_ACCESS_KEY=<random 16-char alphanumeric>
LANGFUSE_S3_SECRET_KEY=<random 32-char alphanumeric>

# Langfuse v3 Headless Init (auto-create org, project, user on first boot)
LANGFUSE_INIT_ORG_ID=<uuid>
LANGFUSE_INIT_ORG_NAME=ai-memory
LANGFUSE_INIT_PROJECT_ID=<uuid>
LANGFUSE_INIT_PROJECT_NAME=ai-memory
LANGFUSE_INIT_PROJECT_PUBLIC_KEY=pk-lf-<random>
LANGFUSE_INIT_PROJECT_SECRET_KEY=sk-lf-<random>
LANGFUSE_INIT_USER_EMAIL=admin@ai-memory.local
LANGFUSE_INIT_USER_NAME=admin
LANGFUSE_INIT_USER_PASSWORD=<random 24-char>
```

#### 2. Start Services

```bash
docker compose -f docker/docker-compose.yml -f docker/docker-compose.langfuse.yml --profile langfuse up -d
```

#### 3. Enable Tracing

```bash
# In docker/.env
LANGFUSE_ENABLED=true
```

#### 4. Verify Health

```bash
# Langfuse Web UI
curl http://localhost:23100/api/public/health

# Check all containers
docker compose -f docker/docker-compose.yml -f docker/docker-compose.langfuse.yml --profile langfuse ps
```

---

## Environment Variables

### Core Tracing

| Variable | Default | Description |
|----------|---------|-------------|
| `LANGFUSE_ENABLED` | `false` | Global kill-switch for all trace emission |
| `LANGFUSE_TRACE_HOOKS` | `true` | Enable/disable hook-level trace emission (independent of `LANGFUSE_ENABLED`) |
| `LANGFUSE_PUBLIC_KEY` | *(empty)* | Langfuse project public key (generated by setup script) |
| `LANGFUSE_SECRET_KEY` | *(empty)* | Langfuse project secret key (generated by setup script) |
| `LANGFUSE_BASE_URL` | `http://localhost:23100` | Langfuse API URL (containers use `http://langfuse-web:3000`) |

### Buffer Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `LANGFUSE_TRACE_BUFFER_MAX_MB` | `100` | Maximum buffer size in MB before eviction kicks in |
| `LANGFUSE_FLUSH_INTERVAL` | `5` | Seconds between flush worker cycles |

### Service Ports

| Variable | Default | Description |
|----------|---------|-------------|
| `LANGFUSE_WEB_PORT` | `23100` | Langfuse Web UI port |
| `LANGFUSE_WORKER_PORT` | `23130` | Langfuse background worker port |
| `LANGFUSE_POSTGRES_PORT` | `25432` | PostgreSQL port |
| `LANGFUSE_CLICKHOUSE_PORT` | `28123` | ClickHouse HTTP port |
| `LANGFUSE_REDIS_PORT` | `26379` | Redis port |
| `LANGFUSE_MINIO_PORT` | `29000` | MinIO S3 port |

---

## Architecture

### Trace Flow

```
Claude Code Session
    â”‚
    â”œâ”€â”€ Hook fires (PostToolUse, SessionStart, etc.)
    â”‚       â”‚
    â”‚       â”œâ”€â”€ emit_trace_event("1_capture", ...)  â”€â”
    â”‚       â”œâ”€â”€ emit_trace_event("2_log", ...)       â”‚
    â”‚       â”œâ”€â”€ emit_trace_event("3_detect", ...)    â”‚
    â”‚       â”œâ”€â”€ emit_trace_event("4_scan", ...)      â”‚  JSON files written to
    â”‚       â”œâ”€â”€ emit_trace_event("5_chunk", ...)     â”‚  ~/.ai-memory/trace_buffer/
    â”‚       â”œâ”€â”€ emit_trace_event("6_embed", ...)     â”‚  (~5ms per event)
    â”‚       â”œâ”€â”€ emit_trace_event("7_store", ...)     â”‚
    â”‚       â””â”€â”€ emit_trace_event("8_enqueue", ...)  â”€â”˜
    â”‚
    â”œâ”€â”€ Classifier Worker processes queue
    â”‚       â””â”€â”€ emit_trace_event("9_classify", ...) â”€â”€â–º trace_buffer/
    â”‚
    â””â”€â”€ Trace Flush Worker (every 5s)
            â”œâ”€â”€ Reads JSON files from trace_buffer/
            â”œâ”€â”€ Groups events by trace_id
            â”œâ”€â”€ Sends to Langfuse via SDK
            â”‚     â”œâ”€â”€ Creates/updates traces (one per pipeline invocation)
            â”‚     â”œâ”€â”€ Creates spans (one per pipeline step)
            â”‚     â””â”€â”€ Links traces to sessions (Claude Code session_id)
            â””â”€â”€ Deletes processed files
```

### Components

| Component | Location | Purpose |
|-----------|----------|---------|
| `trace_buffer.py` | `src/memory/trace_buffer.py` | Fire-and-forget event writer. Atomic JSON file writes (~5ms). Kill-switch checks. Buffer overflow guard. |
| `trace_flush_worker.py` | `src/memory/trace_flush_worker.py` | Long-lived daemon. Reads buffer, batches to Langfuse SDK, evicts old events if buffer grows too large. |
| `langfuse_config.py` | `src/memory/langfuse_config.py` | Langfuse SDK client factory. Reads credentials from environment. |
| `langfuse_setup.sh` | `scripts/langfuse_setup.sh` | Setup script. Generates secrets, starts services, registers custom models. |
| `docker-compose.langfuse.yml` | `docker/docker-compose.langfuse.yml` | Docker Compose overlay with all 7 Langfuse services. |

### Docker Services

All Langfuse services use the `langfuse` profile and join the existing `ai-memory_default` network:

| Service | Image | Purpose |
|---------|-------|---------|
| `langfuse-web` | `langfuse/langfuse:3` | Web UI + REST API |
| `langfuse-worker` | `langfuse/langfuse-worker:3` | Background job processor |
| `langfuse-postgres` | `postgres:17` | Metadata store (projects, users, API keys) |
| `langfuse-clickhouse` | `clickhouse/clickhouse-server:24` | Trace + observation storage (OLAP) |
| `langfuse-redis` | `redis:7` | Job queue and ephemeral cache |
| `langfuse-minio` | `cgr.dev/chainguard/minio` | S3-compatible blob storage for event upload |
| `trace-flush-worker` | Custom (Dockerfile.worker) | Reads trace buffer, flushes to Langfuse |

---

## Pipeline Spans

Each memory operation emits up to 9 spans, representing the full capture-to-classify pipeline:

| Span | Name | Emitted By | Description |
|------|------|-----------|-------------|
| 1 | `1_capture` | `post_tool_capture.py` | Content captured from tool output |
| 2 | `2_log` | `store_async.py` | Activity log entry written |
| 3 | `3_detect` | `store_async.py` | Content type detection (code vs prose) |
| 4 | `4_scan` | `store_async.py` | Security scan (PII/secrets screening) |
| 5 | `5_chunk` | `store_async.py` | Content chunking (512-token chunks, 15% overlap) |
| 6 | `6_embed` | `store_async.py` | Embedding generation (Jina v2) |
| 7 | `7_store` | `store_async.py` | Qdrant upsert (vector + metadata) |
| 8 | `8_enqueue` | `store_async.py` | Classification queue entry |
| 9 | `9_classify` | `process_classification_queue.py` | LLM classification result (type, confidence, provider) |

Additional hook scripts also emit trace events:

| Hook Script | Spans Emitted |
|-------------|---------------|
| `user_prompt_capture.py` | `1_capture` |
| `user_prompt_store_async.py` | `2_log` through `8_enqueue` |
| `agent_response_capture.py` | `1_capture` |
| `agent_response_store_async.py` | `2_log` through `8_enqueue` |
| `error_pattern_capture.py` | `1_capture` |
| `error_store_async.py` | `2_log` through `8_enqueue` |
| `pre_compact_save.py` | `1_capture` |
| `context_injection_tier2.py` | Retrieval spans |

### Span Data

Each span includes:

```json
{
  "event_type": "7_store",
  "trace_id": "abc-123",
  "span_id": "def-456",
  "session_id": "claude-session-xyz",
  "project_id": "my-project",
  "data": {
    "input": { "collection": "code-patterns", "content_hash": "a1b2c3..." },
    "output": { "point_id": "789", "chunks": 3, "dedup_status": "new" },
    "start_time": "2026-02-24T10:30:00Z",
    "end_time": "2026-02-24T10:30:00.045Z"
  }
}
```

---

## Sessions

Traces are automatically grouped by Claude Code session ID using [Langfuse Sessions](https://langfuse.com/docs/observability/features/sessions). This means:

- All memory operations within a single Claude Code conversation appear as one session in Langfuse
- You can see the full timeline of captures, retrievals, and classifications for any conversation
- Session-level metrics (total traces, total spans, error count) are available in the Langfuse UI

The session ID is passed via the `CLAUDE_SESSION_ID` environment variable, which Claude Code sets automatically.

---

## Custom Models

The setup script registers 3 custom model patterns in Langfuse:

| Model Pattern | Match Pattern | Purpose |
|---------------|---------------|---------|
| `ollama/*` | `ollama/.*` | Local Ollama models (free, no API cost) |
| `openrouter/*` | `openrouter/.*` | OpenRouter paid models |
| `openrouter/*:free` | `openrouter/.*:free` | OpenRouter free-tier models |

These enable Langfuse to track classification costs per provider. Models are registered idempotently â€” re-running setup won't create duplicates.

To register additional models manually:

```bash
curl -X POST "http://localhost:23100/api/public/models" \
  -u "pk-lf-...:sk-lf-..." \
  -H "Content-Type: application/json" \
  -d '{"modelName":"my-model","matchPattern":"my-model/.*","inputPrice":0,"outputPrice":0,"unit":"TOKENS"}'
```

---

## Kill-Switch

Langfuse tracing is controlled by two environment variables:

| Variable | Effect |
|----------|--------|
| `LANGFUSE_ENABLED=false` | Disables ALL trace emission. `emit_trace_event()` returns `False` immediately. |
| `LANGFUSE_TRACE_HOOKS=false` | Disables hook-level tracing only. Classifier tracing still works if `LANGFUSE_ENABLED=true`. |

Both checks happen at the top of `emit_trace_event()` with zero overhead when disabled (no imports, no file I/O).

To disable tracing:

```bash
# In docker/.env
LANGFUSE_ENABLED=false

# Restart hooks (they read env on each invocation) â€” no container restart needed
# For classifier, restart the container:
docker restart ai-memory-classifier-worker
```

---

## Buffer Management

### How the Buffer Works

1. Hook scripts call `emit_trace_event()` which writes a JSON file to `~/.ai-memory/trace_buffer/`
2. Files are written atomically (temp file + rename) to prevent partial reads
3. The trace-flush-worker reads files every `LANGFUSE_FLUSH_INTERVAL` seconds (default: 5)
4. Successfully flushed files are deleted; failed files are retried on the next cycle

### Buffer Overflow Protection

The buffer has two overflow protections:

1. **Write-side guard** (`trace_buffer.py`): `emit_trace_event()` checks current buffer size against `LANGFUSE_TRACE_BUFFER_MAX_MB` (default: 100 MB). If the buffer is full, new events are silently dropped.

2. **Read-side eviction** (`trace_flush_worker.py`): The flush worker evicts the oldest trace files when the buffer exceeds the max size. Files are sorted by modification time and deleted oldest-first until the buffer is under the limit.

### Monitoring Buffer Health

```bash
# Check buffer size
du -sh ~/.ai-memory/trace_buffer/

# Count pending events
ls ~/.ai-memory/trace_buffer/*.json 2>/dev/null | wc -l

# Check flush worker logs
docker logs ai-memory-langfuse-trace-flush-worker --tail 20
```

---

## Troubleshooting

### Langfuse UI Shows No Traces

1. **Check kill-switch**: Verify `LANGFUSE_ENABLED=true` in `docker/.env`
2. **Check buffer**: `ls ~/.ai-memory/trace_buffer/*.json` â€” if files exist, the flush worker may not be running
3. **Check flush worker**: `docker logs ai-memory-langfuse-trace-flush-worker --tail 50`
4. **Check API keys**: Verify `LANGFUSE_PUBLIC_KEY` and `LANGFUSE_SECRET_KEY` are set in `.env`

### Flush Worker Can't Connect

```bash
# Verify Langfuse is healthy
curl http://localhost:23100/api/public/health

# Check container networking
docker exec ai-memory-langfuse-trace-flush-worker wget -q -O- http://langfuse-web:3000/api/public/health
```

### Buffer Growing Unbounded

If the buffer keeps growing, the flush worker may be failing silently:

```bash
# Check worker health
docker logs ai-memory-langfuse-trace-flush-worker --tail 100

# Manual flush test â€” verify one event can be sent
curl -X POST "http://localhost:23100/api/public/ingestion" \
  -u "pk-lf-...:sk-lf-..." \
  -H "Content-Type: application/json" \
  -d '{"batch":[{"id":"test","type":"trace-create","body":{"name":"test"}}]}'
```

### Services Won't Start

```bash
# Check for port conflicts
lsof -i :23100  # Langfuse Web
lsof -i :25432  # PostgreSQL
lsof -i :28123  # ClickHouse

# Check Docker logs for specific service
docker logs ai-memory-langfuse-web --tail 50
docker logs ai-memory-langfuse-clickhouse --tail 50

# Verify the ai-memory_default network exists
docker network ls | grep ai-memory
```

### Custom Models Not Registered

```bash
# Check existing models
curl -s "http://localhost:23100/api/public/models" \
  -u "pk-lf-...:sk-lf-..." | python3 -m json.tool

# Re-register manually
./scripts/langfuse_setup.sh --health-check
```

---

## Resource Requirements

The Langfuse stack adds 6 services to the AI Memory core:

| Service | RAM (typical) | CPU | Disk |
|---------|--------------|-----|------|
| Langfuse Web | ~512 MB | 0.5 core | Minimal |
| Langfuse Worker | ~256 MB | 0.5 core | Minimal |
| PostgreSQL | ~256 MB | 0.25 core | ~100 MB (metadata) |
| ClickHouse | ~1 GB | 1 core | Grows with trace volume |
| Redis | ~64 MB | 0.1 core | Ephemeral |
| MinIO | ~128 MB | 0.1 core | Grows with event uploads |
| Trace Flush Worker | ~128 MB | 0.1 core | Minimal |

**Total additional**: ~2.3 GB RAM, 2.5 cores. Combined with AI Memory core (8 services), the full stack requires approximately **32 GiB RAM** and **8 cores**.

---

## Security

- All Langfuse ports are bound to `127.0.0.1` (localhost only) â€” not exposed to the network
- Services use `no-new-privileges` and `cap_drop: ALL` where possible (exceptions: PostgreSQL, ClickHouse, Redis require privilege escalation for user switching)
- Langfuse API keys are stored in `docker/.env` (not committed to git)
- The trace buffer directory uses atomic file writes to prevent data corruption
- MinIO uses a Chainguard hardened distroless image

---

*Documentation for AI Memory v2.0.7 Langfuse LLM Observability integration*
