# AI Memory Module - Docker Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# CONTAINER CONFIGURATION
# =============================================================================
# Container name prefix (for running multiple stacks)
AI_MEMORY_CONTAINER_PREFIX=ai-memory

# Qdrant HTTP API Port (default: 26350)
QDRANT_PORT=26350

# Qdrant gRPC Port (default: 26351)
QDRANT_GRPC_PORT=26351

# Qdrant Log Level (default: INFO)
# Options: TRACE, DEBUG, INFO, WARN, ERROR
QDRANT_LOG_LEVEL=INFO

# Embedding Service Port (default: 28080)
EMBEDDING_PORT=28080

# BMAD Log Level (default: INFO)
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
BMAD_LOG_LEVEL=INFO

# AI Memory Installation Directory (for volume mounts)
# Must match the project installation path
AI_MEMORY_INSTALL_DIR=/path/to/your/project

# =============================================================================
# DOCKER SERVICE AUTHENTICATION (BP-040)
# =============================================================================
# CRITICAL: Never commit real credentials to git. Copy this to .env and customize.
# For production, migrate to Docker Secrets (see BP-040 documentation).

# Grafana Admin Credentials
# Username for Grafana admin user (default: admin)
GRAFANA_ADMIN_USER=admin
# Password for Grafana admin user (set strong password, 20+ chars)
# NOTE: Only applies on FIRST LAUNCH. To change existing:
#   docker exec -it memory-grafana grafana-cli admin reset-admin-password <new_password>
GRAFANA_ADMIN_PASSWORD=change_this_to_secure_password

# Prometheus Basic Auth Password
# Used to generate bcrypt hash in prometheus/web.yml
# Generate hash: python3 -c "import bcrypt; print(bcrypt.hashpw(b'password', bcrypt.gensalt(12)).decode())"
PROMETHEUS_ADMIN_PASSWORD=change_this_to_secure_password

# Prometheus Basic Auth Header (auto-generated by installer)
# Base64-encoded "admin:PROMETHEUS_ADMIN_PASSWORD" for health check auth
# Generate manually: echo -n "admin:YOUR_PASSWORD" | base64, then prefix with "Basic "
PROMETHEUS_BASIC_AUTH_HEADER=Basic YWRtaW46Y2hhbmdlX3RoaXNfdG9fc2VjdXJlX3Bhc3N3b3Jk

# Qdrant API Keys
# Master API key: Full read/write access to Qdrant
# Generate secure key: python3 -c "import secrets, string; print(''.join(secrets.choice(string.ascii_letters + string.digits) for _ in range(24)))"
QDRANT_API_KEY=change_this_to_secure_key
# Read-only API key: Query-only access (optional, for future use)
QDRANT_READ_ONLY_API_KEY=change_this_to_secure_readonly_key

# IMPORTANT Security Notes:
# 1. Qdrant API keys require TLS for production use (we're localhost-only for dev)
# 2. After setting credentials, update prometheus/web.yml with bcrypt hash
# 3. Bind all ports to 127.0.0.1 (already configured) unless using internal Docker networks
# 4. For production: Use Docker Secrets instead of environment variables

# Streamlit Authentication
# Note: Deferred to Phase 2 - requires authentication library installation
# Will implement: streamlit-authenticator or OIDC (BP-040 Section 4)

# =============================================================================
# LLM MEMORY CLASSIFIER CONFIGURATION
# =============================================================================
# Enable/disable memory type classification
MEMORY_CLASSIFIER_ENABLED=true

# =============================================================================
# PROVIDER SELECTION
# =============================================================================
# Primary provider: ollama | openrouter | claude | openai
MEMORY_CLASSIFIER_PRIMARY_PROVIDER=ollama

# Fallback providers (comma-separated)
# Example: openrouter,claude,openai
MEMORY_CLASSIFIER_FALLBACK_PROVIDERS=openrouter

# =============================================================================
# OLLAMA (Local LLM - Free)
# =============================================================================
# URL for Ollama API:
#   - For Docker Desktop (Windows/Mac): http://host.docker.internal:11434 (recommended)
#   - For native Linux Docker: http://172.17.0.1:11434 (Docker bridge gateway)
#   - For WSL without Docker: auto (auto-detects Windows host IP)
#   - For native Linux without Docker: http://localhost:11434
# NOTE: "auto" detection may not work reliably inside Docker containers.
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Recommended models:
#   - sam860/LFM2:2.6b (default, fast)
#   - llama3.2:3b (good quality)
#   - phi3:mini (good reasoning)
OLLAMA_MODEL=sam860/LFM2:2.6b

# =============================================================================
# OPENROUTER (Cloud LLM - Pay per use)
# =============================================================================
# Get your API key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# Recommended models:
#   - google/gemma-2-9b-it:free (free tier)
#   - mistralai/mistral-7b-instruct (cheap)
#   - anthropic/claude-3-haiku (high quality)
OPENROUTER_MODEL=google/gemma-2-9b-it:free

# =============================================================================
# ANTHROPIC CLAUDE (Cloud LLM - High quality)
# =============================================================================
# Get your API key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Recommended models:
#   - claude-3-haiku-20240307 (fastest)
#   - claude-3-5-sonnet-20241022 (best quality/cost)
ANTHROPIC_MODEL=claude-3-haiku-20240307

# =============================================================================
# OPENAI (Cloud LLM)
# =============================================================================
# Get your API key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Recommended models:
#   - gpt-4o-mini (cheapest, good quality)
#   - gpt-4o (best quality)
OPENAI_MODEL=gpt-4o-mini

# =============================================================================
# CLASSIFICATION THRESHOLDS
# =============================================================================
MEMORY_CLASSIFIER_CONFIDENCE_THRESHOLD=0.7
MEMORY_CLASSIFIER_RULE_CONFIDENCE=0.85
MEMORY_CLASSIFIER_MIN_CONTENT_LENGTH=20

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================
MEMORY_CLASSIFIER_TIMEOUT=30
MEMORY_CLASSIFIER_MAX_TOKENS=500
MEMORY_CLASSIFIER_MAX_INPUT_CHARS=4000
