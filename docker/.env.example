# ============================================
# AI-MEMORY DOCKER ENVIRONMENT CONFIGURATION
# ============================================
#
# QUICK SETUP:
#   1. Copy this file: cp .env.example .env
#   2. Generate all secrets (run each command and paste result):
#
#      GRAFANA_SECRET_KEY:
#        python3 -c "import secrets; print(secrets.token_urlsafe(32))"
#
#      GRAFANA_ADMIN_PASSWORD:
#        python3 -c "import secrets; print(secrets.token_urlsafe(16))"
#
#      PROMETHEUS_ADMIN_PASSWORD:
#        python3 -c "import secrets; print(secrets.token_urlsafe(16))"
#
#      PROMETHEUS_BASIC_AUTH_HEADER (after setting password above):
#        echo -n "admin:YOUR_PROMETHEUS_PASSWORD" | base64
#        # Then prefix with "Basic " e.g., Basic YWRtaW46cGFzc3dvcmQ=
#
#      QDRANT_API_KEY:
#        python3 -c "import secrets; print(secrets.token_urlsafe(32))"
#
#   3. Update prometheus/web.yml with bcrypt hash of PROMETHEUS_ADMIN_PASSWORD
#   4. Start services: docker compose --profile monitoring up -d
#
# IMPORTANT SYNC REQUIREMENTS:
#   - PROMETHEUS_ADMIN_PASSWORD ↔ prometheus/web.yml bcrypt hash
#   - PROMETHEUS_ADMIN_PASSWORD ↔ PROMETHEUS_BASIC_AUTH_HEADER
#   - If ANY of these desync, Prometheus auth will fail silently
#
# ============================================

# =============================================================================
# CONTAINER CONFIGURATION
# =============================================================================
# Container name prefix (for running multiple stacks)
AI_MEMORY_CONTAINER_PREFIX=ai-memory

# Qdrant HTTP API Port (default: 26350)
QDRANT_PORT=26350

# Qdrant gRPC Port (default: 26351)
QDRANT_GRPC_PORT=26351

# Qdrant Log Level (default: INFO)
# Options: TRACE, DEBUG, INFO, WARN, ERROR
QDRANT_LOG_LEVEL=INFO

# Embedding Service Port (default: 28080)
EMBEDDING_PORT=28080

# AI Memory Log Level (default: INFO)
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
AI_MEMORY_LOG_LEVEL=INFO

# AI Memory Installation Directory (for volume mounts)
# Must match the project installation path
AI_MEMORY_INSTALL_DIR=/path/to/your/project

# ============================================
# GRAFANA CONFIGURATION
# ============================================
# CRITICAL: Never commit real credentials to git. Copy this to .env and customize.
# For production, migrate to Docker Secrets (see BP-040 documentation).

# Username for Grafana admin user (default: admin)
GRAFANA_ADMIN_USER=admin

# Password for Grafana admin user (set strong password)
# Generate: python3 -c "import secrets; print(secrets.token_urlsafe(16))"
# NOTE: Only applies on FIRST LAUNCH. To change existing:
#   docker exec -it memory-grafana grafana-cli admin reset-admin-password <new_password>
GRAFANA_ADMIN_PASSWORD=

# Grafana cookie encryption key (TECH-DEBT-103/127)
# Used for encrypting session cookies and CSRF tokens. Required for secure multi-instance deployments.
# Generate (32+ chars): python3 -c "import secrets; print(secrets.token_urlsafe(32))"
GRAFANA_SECRET_KEY=

# ============================================
# PROMETHEUS AUTHENTICATION
# ============================================
# IMPORTANT: If you change PROMETHEUS_ADMIN_PASSWORD, you MUST also:
#   1. Regenerate the bcrypt hash in docker/prometheus/web.yml
#   2. Regenerate PROMETHEUS_BASIC_AUTH_HEADER below
#
# To regenerate bcrypt hash:
#   python3 -c "import bcrypt; print(bcrypt.hashpw(b'YOUR_NEW_PASSWORD', bcrypt.gensalt(rounds=12)).decode())"
#
# Then update docker/prometheus/web.yml:
#   basic_auth_users:
#     admin: <paste-new-hash-here>
#
# Generate with: python3 -c "import secrets; print(secrets.token_urlsafe(16))"
PROMETHEUS_ADMIN_PASSWORD=

# Base64-encoded "admin:password" for Prometheus basic auth
# MUST match PROMETHEUS_ADMIN_PASSWORD above
#
# To generate (replace YOUR_PASSWORD with actual password):
#   echo -n "admin:YOUR_PASSWORD" | base64
#
# Or with Python:
#   python3 -c "import base64; print('Basic ' + base64.b64encode(b'admin:YOUR_PASSWORD').decode())"
#
# Format: Basic <base64-encoded-credentials>
PROMETHEUS_BASIC_AUTH_HEADER=

# ============================================
# QDRANT AUTHENTICATION
# ============================================
# Primary API key for read/write operations (required)
# Generate with: python3 -c "import secrets; print(secrets.token_urlsafe(32))"
QDRANT_API_KEY=

# Read-only API key for monitoring and dashboards (optional)
# Purpose: Allows Prometheus/Grafana to scrape Qdrant metrics without write access
# Future use: Will be used for read-only API endpoints and monitoring integrations
# Generate with: python3 -c "import secrets; print(secrets.token_urlsafe(32))"
QDRANT_READ_ONLY_API_KEY=

# ============================================
# OTHER CONFIGURATION
# ============================================
# IMPORTANT Security Notes:
# 1. Qdrant API keys require TLS for production use (we're localhost-only for dev)
# 2. After setting PROMETHEUS_ADMIN_PASSWORD, update prometheus/web.yml with bcrypt hash
# 3. Bind all ports to 127.0.0.1 (already configured) unless using internal Docker networks
# 4. For production: Use Docker Secrets instead of environment variables

# Streamlit Authentication
# Note: Deferred to Phase 2 - requires authentication library installation
# Will implement: streamlit-authenticator or OIDC (BP-040 Section 4)

# =============================================================================
# LLM MEMORY CLASSIFIER CONFIGURATION
# =============================================================================
# Enable/disable memory type classification
MEMORY_CLASSIFIER_ENABLED=true

# =============================================================================
# PROVIDER SELECTION
# =============================================================================
# Primary provider: ollama | openrouter | claude | openai
MEMORY_CLASSIFIER_PRIMARY_PROVIDER=ollama

# Fallback providers (comma-separated)
# Example: openrouter,claude,openai
MEMORY_CLASSIFIER_FALLBACK_PROVIDERS=openrouter

# =============================================================================
# OLLAMA (Local LLM - Free)
# =============================================================================
# URL for Ollama API:
#   - For Docker Desktop (Windows/Mac): http://host.docker.internal:11434 (recommended)
#   - For native Linux Docker: http://172.17.0.1:11434 (Docker bridge gateway)
#   - For WSL without Docker: auto (auto-detects Windows host IP)
#   - For native Linux without Docker: http://localhost:11434
# NOTE: "auto" detection may not work reliably inside Docker containers.
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Recommended models:
#   - sam860/LFM2:2.6b (default, fast)
#   - llama3.2:3b (good quality)
#   - phi3:mini (good reasoning)
OLLAMA_MODEL=sam860/LFM2:2.6b

# =============================================================================
# OPENROUTER (Cloud LLM - Pay per use)
# =============================================================================
# Get your API key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# Recommended models:
#   - google/gemma-2-9b-it:free (free tier)
#   - mistralai/mistral-7b-instruct (cheap)
#   - anthropic/claude-3-haiku (high quality)
OPENROUTER_MODEL=google/gemma-2-9b-it:free

# =============================================================================
# ANTHROPIC CLAUDE (Cloud LLM - High quality)
# =============================================================================
# Get your API key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Recommended models:
#   - claude-3-haiku-20240307 (fastest)
#   - claude-3-5-sonnet-20241022 (best quality/cost)
ANTHROPIC_MODEL=claude-3-haiku-20240307

# =============================================================================
# OPENAI (Cloud LLM)
# =============================================================================
# Get your API key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Recommended models:
#   - gpt-4o-mini (cheapest, good quality)
#   - gpt-4o (best quality)
OPENAI_MODEL=gpt-4o-mini

# =============================================================================
# CLASSIFICATION THRESHOLDS
# =============================================================================
MEMORY_CLASSIFIER_CONFIDENCE_THRESHOLD=0.7
MEMORY_CLASSIFIER_RULE_CONFIDENCE=0.85
MEMORY_CLASSIFIER_MIN_CONTENT_LENGTH=20

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================
MEMORY_CLASSIFIER_TIMEOUT=30
MEMORY_CLASSIFIER_MAX_TOKENS=500
MEMORY_CLASSIFIER_MAX_INPUT_CHARS=4000

# ============================================================
# v2.0.6 Settings — Intent + Git History + Semantic Decay
# ============================================================

# --- Decay Scoring (Foundation) ---
DECAY_ENABLED=true
DECAY_SEMANTIC_WEIGHT=0.7
DECAY_HALF_LIFE_CODE_PATTERNS=14
DECAY_HALF_LIFE_DISCUSSIONS=21
DECAY_HALF_LIFE_CONVENTIONS=60
DECAY_HALF_LIFE_JIRA_DATA=30
# DECAY_MIN_SCORE=0.1  # Reserved for Phase 3 — not enforced yet
# DECAY_TYPE_OVERRIDES=github_ci_result:7,github_code_blob:14,github_commit:14,conversation:21,session_summary:21,github_issue:30,github_pr:30,jira_issue:30,agent_memory:30,agent_handoff:30,guideline:60,rule:60,architecture_decision:90  # Advanced: per-type half-life overrides

# --- Audit Trail (Foundation) ---
# AUDIT_DIR=.audit  # Advanced: project-local audit directory (default: .audit)

# --- Automated Updates (Foundation) ---
AUTO_UPDATE_ENABLED=true

# GITHUB INTEGRATION (Optional — PLAN-006 Phase 1a)
# GITHUB_SYNC_ENABLED=false
# GITHUB_TOKEN=ghp_your_fine_grained_token_here
# GITHUB_REPO=owner/repo
# GITHUB_SYNC_INTERVAL=1800
# GITHUB_BRANCH=main
# GITHUB_CODE_BLOB_ENABLED=true
# GITHUB_CODE_BLOB_MAX_SIZE=102400
# GITHUB_CODE_BLOB_EXCLUDE=node_modules,*.min.js,.git,__pycache__,*.pyc,build,dist,*.egg-info
# GITHUB_SYNC_ON_START=true

# =============================================================================
# ENCRYPTION (SPEC-011)
# =============================================================================
# Secrets Backend (v2.0.6)
# Options: sops-age, keyring, env-file
# - sops-age: SOPS+age encryption (recommended, secrets encrypted in Git)
# - keyring: System keyring (OS-level encryption, no extra tools)
# - env-file: Plaintext .env file (minimum security)
AI_MEMORY_SECRETS_BACKEND=env-file

# =============================================================================
# DUAL EMBEDDING (SPEC-010)
# =============================================================================
# Embedding Models (v2.0.6)
# MODEL_NAME is honored as fallback for MODEL_NAME_EN (backward compat)
MODEL_NAME_EN=jinaai/jina-embeddings-v2-base-en
MODEL_NAME_CODE=jinaai/jina-embeddings-v2-base-code
EMBEDDING_MODEL_DENSE_EN=jinaai/jina-embeddings-v2-base-en
EMBEDDING_MODEL_DENSE_CODE=jinaai/jina-embeddings-v2-base-code
# Increase memory limit for dual model support (loading two ONNX models requires ~3GB)
EMBEDDING_MEMORY_LIMIT=4G

# =============================================================================
# PROGRESSIVE CONTEXT INJECTION (SPEC-012)
# =============================================================================
# Enable/disable progressive injection (Tier 1 + Tier 2)
INJECTION_ENABLED=true

# Tier 1 Bootstrap Budget (startup trigger, 2-3K tokens recommended)
BOOTSTRAP_TOKEN_BUDGET=2500

# Tier 2 Confidence Gate (skip injection if best score < threshold)
INJECTION_CONFIDENCE_THRESHOLD=0.6

# Tier 2 Adaptive Budget Range (500-1500 tokens recommended)
INJECTION_BUDGET_FLOOR=500
INJECTION_BUDGET_CEILING=1500

# Advanced: Adaptive Budget Signal Weights (must sum to 1.0)
# INJECTION_QUALITY_WEIGHT=0.5   # Weight for retrieval quality signal
# INJECTION_DENSITY_WEIGHT=0.3   # Weight for result density signal
# INJECTION_DRIFT_WEIGHT=0.2     # Weight for topic drift signal

# =============================================================================
# FRESHNESS DETECTION (SPEC-013)
# =============================================================================
# Enable/disable freshness detection for code-patterns memories
FRESHNESS_ENABLED=true

# Tier Classification Thresholds (commits since memory stored)
FRESHNESS_COMMIT_THRESHOLD_AGING=3      # Classify as "aging" (some activity)
FRESHNESS_COMMIT_THRESHOLD_STALE=10     # Classify as "stale" (significant activity)
FRESHNESS_COMMIT_THRESHOLD_EXPIRED=25   # Classify as "expired" (high churn)


# =============================================================================
# SECURITY SCANNING (SPEC-009)
# =============================================================================
# Master toggle for scanning pipeline
SECURITY_SCANNING_ENABLED=true

# Enable SpaCy NER layer (Layer 3) - requires SpaCy installed
SECURITY_SCANNING_NER_ENABLED=true

# Block content with detected secrets (if False, mask instead)
SECURITY_BLOCK_ON_SECRETS=true

# Scan mode for GitHub content: relaxed (skip detect-secrets), strict (full scan), off (no scan)
SECURITY_SCAN_GITHUB_MODE=relaxed

# Scan mode for session content (user prompts, agent responses):
# relaxed (skip detect-secrets Layer 2), strict (full scan), off (no scan)
SECURITY_SCAN_SESSION_MODE=relaxed

# =============================================================================
# PARZIVAL SESSION AGENT (SPEC-015)
# =============================================================================
# Enable Parzival session agent (set by installer)
PARZIVAL_ENABLED=false

# User's display name for Parzival greeting and handoffs
PARZIVAL_USER_NAME=Developer

# Communication language for Parzival
PARZIVAL_LANGUAGE=English

# Language for generated documents (handoffs, specs)
PARZIVAL_DOC_LANGUAGE=English

# Project-relative path to oversight directory
PARZIVAL_OVERSIGHT_FOLDER=oversight

# Number of recent handoff files to keep
PARZIVAL_HANDOFF_RETENTION=10
