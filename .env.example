# .env.example - AI Memory Module Configuration
# Copy to .env and modify as needed
# Source: Story 7.4 - Environment Variable Configuration

# =============================================================================
# Core Thresholds (FR42)
# =============================================================================

# Minimum similarity score for retrieval (0.0-1.0)
# Lower = more results, potentially less relevant
# Default: 0.7
SIMILARITY_THRESHOLD=0.7

# Similarity threshold for deduplication (0.80-0.99)
# Higher = stricter dedup, fewer similar memories stored
# Default: 0.95
DEDUP_THRESHOLD=0.95

# Maximum memories to retrieve per session (1-50)
# Default: 5
MAX_RETRIEVALS=5

# Token budget for context injection (100-100000)
# Controls how much context is sent to Claude
# Default: 4000 (increased per BP-039 Section 3: "Target 50% context window utilization")
TOKEN_BUDGET=4000

# =============================================================================
# Service Configuration
# =============================================================================

# Qdrant connection
# Default: localhost:26350 (Story 1.1)
QDRANT_HOST=localhost
QDRANT_PORT=26350

# Optional Qdrant API key for authentication
# Leave empty for local development
# QDRANT_API_KEY=

# Embedding service
# Default: localhost:28080 (DEC-004)
EMBEDDING_HOST=localhost
EMBEDDING_PORT=28080

# Monitoring API
# Default: localhost:28000
MONITORING_HOST=localhost
MONITORING_PORT=28000

# =============================================================================
# Logging & Monitoring
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO

# Log format: json (production), text (development)
# Default: json
LOG_FORMAT=json

# Collection size warning threshold
# Default: 10000
COLLECTION_SIZE_WARNING=10000

# Collection size critical threshold
# Default: 50000
COLLECTION_SIZE_CRITICAL=50000

# =============================================================================
# Pushgateway Configuration (TECH-DEBT-008)
# =============================================================================

# Enable pushgateway metrics for short-lived hook processes
# Set to false to disable metrics pushing
# Default: true
PUSHGATEWAY_ENABLED=true

# Pushgateway URL for metrics push
# Used by hooks to push execution metrics
# Default: localhost:29091
PUSHGATEWAY_URL=localhost:29091

# =============================================================================
# Paths
# =============================================================================

# Installation directory
# Default: ~/.ai-memory
# INSTALL_DIR=$HOME/.ai-memory

# Queue file for pending operations
# Default: ~/.claude-memory/pending_queue.jsonl
# QUEUE_PATH=$HOME/.claude-memory/pending_queue.jsonl

# Session logs
# Default: ~/.claude-memory/sessions.jsonl
# SESSION_LOG_PATH=$HOME/.claude-memory/sessions.jsonl

# =============================================================================
# MEMORY CLASSIFIER CONFIGURATION (TECH-DEBT-069)
# =============================================================================

# Enable/disable LLM-based classification
# Default: true
MEMORY_CLASSIFIER_ENABLED=true

# Warm up model on startup (pre-load for faster first classification)
# Default: true
MEMORY_CLASSIFIER_WARM_UP=true

# Run classification asynchronously in background
# Default: true
MEMORY_CLASSIFIER_ASYNC=true

# Queue failed classifications for retry
# Default: true
MEMORY_CLASSIFIER_QUEUE_ON_FAILURE=true

# --- Provider Selection ---
# Primary provider: ollama (free, local), openrouter (cheap), or claude (reliable)
# Default: ollama
MEMORY_CLASSIFIER_PRIMARY_PROVIDER=ollama

# Fallback providers (comma-separated)
# Default: openrouter,claude
MEMORY_CLASSIFIER_FALLBACK_PROVIDERS=openrouter,claude

# --- Thresholds ---
# Minimum confidence to change memory type (0.0-1.0)
# Default: 0.7
MEMORY_CLASSIFIER_CONFIDENCE_THRESHOLD=0.7

# Minimum confidence for rule-based classification to skip LLM (0.0-1.0)
# Default: 0.85
MEMORY_CLASSIFIER_RULE_CONFIDENCE=0.85

# Minimum content length to classify (characters)
# Default: 20
MEMORY_CLASSIFIER_MIN_CONTENT_LENGTH=20

# --- Performance ---
# Timeout per provider attempt (seconds)
# Default: 10
MEMORY_CLASSIFIER_TIMEOUT=10

# Maximum output tokens from LLM
# Default: 500
MEMORY_CLASSIFIER_MAX_TOKENS=500

# Maximum input characters (longer content is truncated)
# Default: 4000
MEMORY_CLASSIFIER_MAX_INPUT_CHARS=4000

# Rate limit (max classifications per minute)
# Default: 60
MEMORY_CLASSIFIER_RATE_LIMIT=60

# --- Ollama Configuration (free, local) ---
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# --- OpenRouter Configuration (cheap fallback) ---
# Get API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=anthropic/claude-3-haiku

# --- Claude/Anthropic Configuration (reliable fallback) ---
# Uses existing ANTHROPIC_API_KEY if already set
# ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-haiku-20241022
