#!/usr/bin/env python3
"""UserPromptSubmit Hook - Retrieve best practices when user asks about conventions.

Memory System V2.0 Phase 3: Trigger System
Automatically retrieves relevant best practices when user asks questions about conventions,
coding standards, patterns, or "how should I" questions.

Signal Detection:
    - UserPromptSubmit hook
    - Keywords: "best practice", "coding standard", "convention", "how should i", etc.
    - Uses detect_best_practices_keywords() for detection

Action:
    - Search conventions collection
    - Query based on extracted topic
    - Inject up to 3 best practices to stdout

Configuration:
    - Hook: UserPromptSubmit
    - Collection: conventions
    - Max results: 3

Architecture:
    UserPromptSubmit â†’ Extract prompt â†’ detect_best_practices_keywords() â†’ Extract topic
    â†’ Search conventions â†’ Format results â†’ Output to stdout â†’ Claude sees context

Exit Codes:
    - 0: Success (or graceful degradation)
"""

import json
import logging
import os
import sys
import time
from pathlib import Path

# Add src to path for imports
INSTALL_DIR = os.environ.get('AI_MEMORY_INSTALL_DIR', os.path.expanduser('~/.ai-memory'))
sys.path.insert(0, os.path.join(INSTALL_DIR, "src"))

# Configure structured logging using shared utility
from memory.config import COLLECTION_CONVENTIONS
from memory.hooks_common import (
    get_metrics,
    get_trigger_metrics,
    log_to_activity,
    setup_hook_logging,
)

logger = setup_hook_logging()

# CR-2 FIX: Use consolidated metrics import
memory_retrievals_total, retrieval_duration_seconds, hook_duration_seconds = get_metrics()

# TECH-DEBT-067: V2.0 trigger metrics
trigger_fires_total, trigger_results_returned = get_trigger_metrics()

# Display formatting constants
MAX_CONTENT_CHARS = 500  # Maximum characters to show in context output (standardized LOW-1)


def format_best_practice(practice: dict, index: int) -> str:
    """Format a single best practice for display.

    Args:
        practice: Practice dict with content, score, type
        index: 1-based index for numbering

    Returns:
        Formatted string for stdout display
    """
    content = practice.get("content", "")
    score = practice.get("score", 0)
    practice_type = practice.get("type", "guideline")
    tags = practice.get("tags", [])

    # Build header with relevance
    header = f"{index}. **{practice_type}** ({score:.0%}) [conventions]"
    if tags:
        header += f" - {', '.join(tags[:3])}"

    # Truncate content if too long
    if len(content) > MAX_CONTENT_CHARS:
        content = content[:MAX_CONTENT_CHARS] + "..."

    return f"{header}\n{content}\n"


def main() -> int:
    """UserPromptSubmit hook entry point.

    Reads hook input from stdin, detects best practices keywords in user prompt,
    searches conventions collection, and outputs to stdout.

    Returns:
        Exit code: Always 0 (graceful degradation)
    """
    start_time = time.perf_counter()

    try:
        # Parse hook input from stdin
        try:
            hook_input = json.load(sys.stdin)
        except json.JSONDecodeError:
            logger.warning("malformed_hook_input")
            return 0

        # Extract context
        cwd = hook_input.get("cwd", os.getcwd())
        user_input = hook_input.get("prompt", "")  # FIX: UserPromptSubmit uses "prompt", not "user_input"

        if not user_input:
            logger.debug("no_user_input")
            return 0

        # Detect best practices keywords (trigger condition)
        from memory.triggers import TRIGGER_CONFIG, detect_best_practices_keywords

        topic = detect_best_practices_keywords(user_input)
        if not topic:
            # No best practices keywords detected
            logger.debug("no_best_practices_keywords", extra={"user_input": user_input[:50]})
            return 0

        # Best practices keywords detected - retrieve conventions
        config = TRIGGER_CONFIG.get("best_practices_keywords", {})
        if not config.get("enabled", False):
            logger.debug("best_practices_keyword_trigger_disabled")
            return 0

        # Build query from extracted topic
        query = f"Best practices for {topic}"

        # Search conventions collection
        from memory.config import get_config
        from memory.health import check_qdrant_health
        from memory.project import detect_project
        from memory.qdrant_client import get_qdrant_client
        from memory.search import MemorySearch

        mem_config = get_config()
        client = get_qdrant_client(mem_config)

        # Check Qdrant health
        if not check_qdrant_health(client):
            logger.warning("qdrant_unavailable")
            if memory_retrievals_total:
                memory_retrievals_total.labels(collection=COLLECTION_CONVENTIONS, status="failed").inc()
            return 0

        # Detect project for logging
        project_name = detect_project(cwd)

        # Search for relevant best practices
        search = MemorySearch(mem_config)
        try:
            # Conventions are shared across projects (group_id=None)
            # CR-2 FIX: Add type filter for precision (rule, guideline only)
            results = search.search(
                query=query,
                collection=COLLECTION_CONVENTIONS,
                group_id=None,  # Conventions are global
                limit=config.get("max_results", 3),
                score_threshold=mem_config.similarity_threshold,
                memory_type=["rule", "guideline"]  # CR-2 FIX: Filter to relevant types
            )

            if not results:
                # No relevant practices found
                duration_ms = (time.perf_counter() - start_time) * 1000
                log_to_activity(f"ðŸŽ¯ Best practices: No results for '{topic[:50]}'", INSTALL_DIR)
                logger.debug("no_best_practices_found", extra={
                    "topic": topic[:50],
                    "query": query[:50],
                    "duration_ms": round(duration_ms, 2)
                })
                if memory_retrievals_total:
                    memory_retrievals_total.labels(collection=COLLECTION_CONVENTIONS, status="empty").inc()
                # TECH-DEBT-067: Track trigger fires with status="empty"
                if trigger_fires_total:
                    trigger_fires_total.labels(trigger_type="best_practices_keywords", status="empty", project=project_name).inc()
                if trigger_results_returned:
                    trigger_results_returned.labels(trigger_type="best_practices_keywords").observe(0)

                # TECH-DEBT-067 Part 2: Push metrics to Pushgateway
                from memory.metrics_push import push_trigger_metrics
                push_trigger_metrics(
                    trigger_type="best_practices_keywords",
                    status="empty",
                    project=project_name,
                    results_count=0,
                    duration_seconds=(time.perf_counter() - start_time)
                )
                return 0

            # Format for stdout display
            output_parts = []
            output_parts.append("\n" + "="*70)
            output_parts.append("ðŸŽ¯ RELEVANT BEST PRACTICES")
            output_parts.append("="*70)
            output_parts.append(f"Question: {user_input[:100]}")
            output_parts.append(f"Topic: {topic[:80]}")
            output_parts.append("")

            for i, practice in enumerate(results, 1):
                output_parts.append(format_best_practice(practice, i))

            output_parts.append("="*70 + "\n")

            # Output to stdout (Claude sees this with the prompt)
            print("\n".join(output_parts))

            # Log success
            duration_ms = (time.perf_counter() - start_time) * 1000
            log_to_activity(f"ðŸŽ¯ Best practices retrieved for '{topic[:50]}' [{duration_ms:.0f}ms]", INSTALL_DIR)
            logger.info("best_practices_keywords_retrieved", extra={
                "topic": topic[:50],
                "results_count": len(results),
                "duration_ms": round(duration_ms, 2),
                "project": project_name
            })

            # Metrics
            if memory_retrievals_total:
                memory_retrievals_total.labels(collection=COLLECTION_CONVENTIONS, status="success").inc()
            if retrieval_duration_seconds:
                retrieval_duration_seconds.observe(duration_ms / 1000.0)
            if hook_duration_seconds:
                hook_duration_seconds.labels(hook_type="UserPromptSubmit_BestPractices").observe(duration_ms / 1000.0)
            # TECH-DEBT-067: Track trigger fires and results
            if trigger_fires_total:
                trigger_fires_total.labels(trigger_type="best_practices_keywords", status="success", project=project_name).inc()
            if trigger_results_returned:
                trigger_results_returned.labels(trigger_type="best_practices_keywords").observe(len(results))

            # TECH-DEBT-067 Part 2: Push metrics to Pushgateway
            from memory.metrics_push import push_trigger_metrics
            push_trigger_metrics(
                trigger_type="best_practices_keywords",
                status="success",
                project=project_name,
                results_count=len(results),
                duration_seconds=duration_seconds
            )

        finally:
            search.close()

        return 0

    except Exception as e:
        # Catch-all error handler - always gracefully degrade
        logger.error("hook_failed", extra={
            "error": str(e),
            "error_type": type(e).__name__
        })

        # Metrics
        if memory_retrievals_total:
            memory_retrievals_total.labels(collection=COLLECTION_CONVENTIONS, status="failed").inc()
        if hook_duration_seconds:
            duration_seconds = time.perf_counter() - start_time
            hook_duration_seconds.labels(hook_type="UserPromptSubmit_BestPractices").observe(duration_seconds)

        return 0  # Always exit 0 - graceful degradation


if __name__ == "__main__":
    sys.exit(main())
